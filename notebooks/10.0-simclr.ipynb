{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "!pip install -qU pytorch-lightning\n",
    "!pip install -qU lightning-bolts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from typing import Optional\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import CSVLogger, WandbLogger\n",
    "\n",
    "from pl_bolts.models.self_supervised.resnets import resnet50\n",
    "from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR\n",
    "from pl_bolts.metrics import mean, accuracy\n",
    "\n",
    "from pl_bolts.models.self_supervised.evaluator import SSLEvaluator\n",
    "from pl_bolts.transforms.dataset_normalizations import imagenet_normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dfx, image_dir, transform=None):\n",
    "        \"\"\"Create a pytorch dataser\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dfx : (DataFrame), DataFrame containing image name and retinopathy grade\n",
    "        image_dir : (str), path of the image directory\n",
    "        transform : (Albumentations, optional), Transformations. Defaults to None.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dfx = dfx\n",
    "        self.image_ids = self.dfx.iloc[:,0].values\n",
    "        self.targets = self.dfx.iloc[:,1].values\n",
    "        self.num_classes = self.dfx.iloc[:,1].nunique()\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.image_ids.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_ids[idx]\n",
    "        index = torch.tensor(self.targets[idx])\n",
    "\n",
    "        img = Image.open(os.path.join(self.image_dir, img_name + '.jpg')).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLRTrainDataTransform(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_height: int = 224,\n",
    "        gaussian_blur: bool = False,\n",
    "        jitter_strength: float = 1.,\n",
    "        normalize: Optional[transforms.Normalize] = None\n",
    "    ) -> None:\n",
    "\n",
    "        self.jitter_strength = jitter_strength\n",
    "        self.input_height = input_height\n",
    "        self.gaussian_blur = gaussian_blur\n",
    "        self.normalize = normalize\n",
    "\n",
    "        self.color_jitter = transforms.ColorJitter(\n",
    "            0.8 * self.jitter_strength,\n",
    "            0.8 * self.jitter_strength,\n",
    "            0.8 * self.jitter_strength,\n",
    "            0.2 * self.jitter_strength\n",
    "        )\n",
    "\n",
    "        data_transforms = [\n",
    "            transforms.RandomResizedCrop(size=self.input_height),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomApply([self.color_jitter], p=0.8),\n",
    "            transforms.RandomGrayscale(p=0.2)\n",
    "        ]\n",
    "\n",
    "        if self.gaussian_blur:\n",
    "            data_transforms.append(GaussianBlur(kernel_size=int(0.1 * self.input_height, p=0.5)))\n",
    "\n",
    "        data_transforms.append(transforms.ToTensor())\n",
    "\n",
    "        if self.normalize:\n",
    "            data_transforms.append(normalize)\n",
    "\n",
    "        self.train_transform = transforms.Compose(data_transforms)\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        transform = self.train_transform\n",
    "\n",
    "        xi = transform(sample)\n",
    "        xj = transform(sample)\n",
    "\n",
    "        return xi, xj\n",
    "\n",
    "\n",
    "class SimCLREvalDataTransform(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_height: int = 224,\n",
    "        normalize: Optional[transforms.Normalize] = None\n",
    "    ):\n",
    "        self.input_height = input_height\n",
    "        self.normalize = normalize\n",
    "\n",
    "        data_transforms = [\n",
    "            transforms.Resize((self.input_height,self.input_height)),\n",
    "            transforms.ToTensor()\n",
    "        ]\n",
    "\n",
    "        if self.normalize:\n",
    "            data_transforms.append(normalize)\n",
    "\n",
    "        self.test_transform = transforms.Compose(data_transforms)\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        transform = self.test_transform\n",
    "\n",
    "        xi = transform(sample)\n",
    "        xj = transform(sample)\n",
    "\n",
    "        return xi, xj\n",
    "\n",
    "\n",
    "class GaussianBlur(object):\n",
    "    # Implements Gaussian blur as described in the SimCLR paper\n",
    "    def __init__(self, kernel_size, p=0.5, min=0.1, max=2.0):\n",
    "        self.min = min\n",
    "        self.max = max\n",
    "\n",
    "        # kernel size is set to be 10% of the image height/width\n",
    "        self.kernel_size = kernel_size\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        sample = np.array(sample)\n",
    "\n",
    "        # blur the image with a 50% chance\n",
    "        prob = np.random.random_sample()\n",
    "\n",
    "        if prob < self.p:\n",
    "            sigma = (self.max - self.min) * np.random.random_sample() + self.min\n",
    "            sample = cv2.GaussianBlur(sample, (self.kernel_size, self.kernel_size), sigma)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, df_train, df_valid,\n",
    "                 train_path, valid_path,\n",
    "                 train_transforms, val_transforms,\n",
    "                 num_workers=2,\n",
    "                 batch_size=32):\n",
    "        \"\"\"Pytorch Lightning Data Module\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df_train : (DataFrame), Train dataframe\n",
    "        df_valid : (DataFrame), Val dataframe\n",
    "        df_test  : (DataFrame), Test dataframe\n",
    "        train_path : (str), Train path\n",
    "        valid_path : (str), Val path\n",
    "        test_path : (str), Test path\n",
    "        train_transforms : (Albumentation), Train transformations\n",
    "        val_transforms : (Albumentation), Val transformations\n",
    "        test_transforms : (Albumentation), Test transformations\n",
    "        num_workers : (int, optional), Num workers. Defaults to 2.\n",
    "        batch_size : (int, optional), Batch Size. Defaults to 32.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.train_data = CustomDataset(df_train,\n",
    "                                        train_path,\n",
    "                                        transform=train_transforms)\n",
    "        \n",
    "        self.val_data = CustomDataset(df_valid,\n",
    "                                      valid_path,\n",
    "                                      transform=val_transforms)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data,\n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers,\n",
    "                          shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_data,\n",
    "                          batch_size=self.batch_size,\n",
    "                          shuffle=False,\n",
    "                          num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "PATH = '../data/processed/'\n",
    "IMG_PATH = '../data/processed/'\n",
    "dfx = pd.read_csv(PATH+'2.Groundtruths/a.IDRiD_Disease_Grading_Training_Labels.csv',usecols = ['Image name','Retinopathy grade'])\n",
    "df_test = pd.read_csv(PATH+'2.Groundtruths/b.IDRiD_Disease_Grading_Testing_Labels.csv',usecols = ['Image name','Retinopathy grade'])\n",
    "df_train, df_valid = train_test_split(\n",
    "    dfx, test_size=0.2, random_state=42, stratify=dfx['Retinopathy grade'].values\n",
    ")\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_valid = df_valid.reset_index(drop=True)\n",
    "\n",
    "dm = DRDataModule(dfx, df_test,\n",
    "                  IMG_PATH+'images_resized',\n",
    "                  IMG_PATH+'test_images_resized',\n",
    "                  SimCLRTrainDataTransform(),\n",
    "                  SimCLREvalDataTransform(),\n",
    "                  num_workers=0,\n",
    "                  batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (img1,img2),_ in dm.train_dataloader():\n",
    "    for i1,i2 in zip(img1,img2):\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.imshow(i1.permute(1,2,0))\n",
    "        plt.axis('off')\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.imshow(i2.permute(1,2,0))\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (img1,img2),_ in dm.val_dataloader():\n",
    "    for i1,i2 in zip(img1,img2):\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.imshow(i1.permute(1,2,0))\n",
    "        plt.axis('off')\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.imshow(i2.permute(1,2,0))\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nt_xent_loss(out_1, out_2, temperature):\n",
    "    out = torch.cat([out_1, out_2], dim=0)\n",
    "    n_samples = len(out)\n",
    "\n",
    "    # Full similarity matrix\n",
    "    cov = torch.mm(out, out.t().contiguous())\n",
    "    sim = torch.exp(cov / temperature)\n",
    "\n",
    "    mask = ~torch.eye(n_samples, device=sim.device).bool()\n",
    "    neg = sim.masked_select(mask).view(n_samples, -1).sum(dim=-1)\n",
    "\n",
    "    # Positive similarity\n",
    "    pos = torch.exp(torch.sum(out_1 * out_2, dim=-1) / temperature)\n",
    "    pos = torch.cat([pos, pos], dim=0)\n",
    "\n",
    "    loss = -torch.log(pos / neg).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Projection(nn.Module):\n",
    "    def __init__(self, input_dim=2048, hidden_dim=2048, output_dim=128):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            # Flatten(),\n",
    "            nn.Linear(self.input_dim, self.hidden_dim, bias=True),\n",
    "            nn.BatchNorm1d(self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim, self.output_dim, bias=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return F.normalize(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLR(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 batch_size,\n",
    "                 num_samples,\n",
    "                 warmup_epochs=10,\n",
    "                 lr=1e-4,\n",
    "                 opt_weight_decay=1e-6,\n",
    "                 loss_temperature=0.5,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch_size: the batch size\n",
    "            num_samples: num samples in the dataset\n",
    "            warmup_epochs: epochs to warmup the lr for\n",
    "            lr: the optimizer learning rate\n",
    "            opt_weight_decay: the optimizer weight decay\n",
    "            loss_temperature: the loss temperature\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.nt_xent_loss = nt_xent_loss\n",
    "        self.encoder = self.init_encoder()\n",
    "\n",
    "        # h -> || -> z\n",
    "        self.projection = Projection()\n",
    "\n",
    "    def init_encoder(self):\n",
    "        encoder = resnet50(return_all_feature_maps=False)\n",
    "        return encoder\n",
    "\n",
    "    def exclude_from_wt_decay(self, named_params, weight_decay, skip_list=['bias', 'bn']):\n",
    "        params = []\n",
    "        excluded_params = []\n",
    "\n",
    "        for name, param in named_params:\n",
    "            if not param.requires_grad:\n",
    "                continue\n",
    "            elif any(layer_name in name for layer_name in skip_list):\n",
    "                excluded_params.append(param)\n",
    "            else:\n",
    "                params.append(param)\n",
    "\n",
    "        return [\n",
    "            {'params': params, 'weight_decay': weight_decay},\n",
    "            {'params': excluded_params, 'weight_decay': 0.}\n",
    "        ]\n",
    "\n",
    "    def setup(self, stage):\n",
    "        global_batch_size = self.trainer.world_size * self.hparams.batch_size\n",
    "        self.train_iters_per_epoch = self.hparams.num_samples // global_batch_size\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # TRICK 1 (Use lars + filter weights)\n",
    "        # exclude certain parameters\n",
    "        parameters = self.exclude_from_wt_decay(\n",
    "            self.named_parameters(),\n",
    "            weight_decay=self.hparams.opt_weight_decay\n",
    "        )\n",
    "\n",
    "        optimizer = Adam(parameters, lr=self.hparams.lr)\n",
    "\n",
    "        # Trick 2 (after each step)\n",
    "        self.hparams.warmup_epochs = self.hparams.warmup_epochs * self.train_iters_per_epoch\n",
    "        max_epochs = self.trainer.max_epochs * self.train_iters_per_epoch\n",
    "\n",
    "        linear_warmup_cosine_decay = LinearWarmupCosineAnnealingLR(\n",
    "            optimizer,\n",
    "            warmup_epochs=self.hparams.warmup_epochs,\n",
    "            max_epochs=max_epochs,\n",
    "            warmup_start_lr=0,\n",
    "            eta_min=0\n",
    "        )\n",
    "\n",
    "        scheduler = {\n",
    "            'scheduler': linear_warmup_cosine_decay,\n",
    "            'interval': 'step',\n",
    "            'frequency': 1\n",
    "        }\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def forward(self, x):\n",
    "        if isinstance(x, list):\n",
    "            x = x[0]\n",
    "\n",
    "        result = self.encoder(x)\n",
    "        if isinstance(result, list):\n",
    "            result = result[-1]\n",
    "        return result\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.shared_step(batch, batch_idx)\n",
    "\n",
    "        self.log('train_loss', loss,  prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.shared_step(batch, batch_idx)\n",
    "\n",
    "        self.log('avg_val_loss', loss,  prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def shared_step(self, batch, batch_idx):\n",
    "        (img1, img2), y = batch\n",
    "\n",
    "        # ENCODE\n",
    "        # encode -> representations\n",
    "        # (b, 3, 32, 32) -> (b, 2048, 2, 2)\n",
    "        h1 = self.encoder(img1)\n",
    "        h2 = self.encoder(img2)\n",
    "\n",
    "        # the bolts resnets return a list of feature maps\n",
    "        if isinstance(h1, list):\n",
    "            h1 = h1[-1]\n",
    "            h2 = h2[-1]\n",
    "\n",
    "        # PROJECT\n",
    "        # img -> E -> h -> || -> z\n",
    "        # (b, 2048, 2, 2) -> (b, 128)\n",
    "        z1 = self.projection(h1)\n",
    "        z2 = self.projection(h2)\n",
    "\n",
    "        loss = self.nt_xent_loss(z1, z2, self.hparams.loss_temperature)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pl_bolts.models.self_supervised.evaluator import SSLEvaluator\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "class SSLOnlineEvaluator(pl.Callback):\n",
    "    def __init__(self,drop_p=0.2,hidden_dim=1024,z_dim=2048,num_classes=5):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.drop_p = drop_p\n",
    "        self.optimizer = None\n",
    "        self.z_dim = z_dim\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "    def on_pretrain_routine_start(self, trainer, pl_module):\n",
    "        \n",
    "        pl_module.non_linear_eval = SSLEvaluator(\n",
    "            n_input = self.z_dim,\n",
    "            n_classes = self.num_classes,\n",
    "            p = self.drop_p\n",
    "        ).to(pl_module.device)\n",
    "        \n",
    "        self.optimizer = torch.optim.SGD(pl_module.non_linear_eval.parameters(), lr=1e-3)\n",
    "        \n",
    "    def on_train_batch_end(self, pl_module, outputs, batch, batch_idx):\n",
    "        # self, self.lightning_module, outputs, batch, batch_idx\n",
    "        (x,_),y = batch\n",
    "        x = x.to(pl_module.device)\n",
    "        y = y.to(pl_module.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            rep = pl_module(x)\n",
    "        \n",
    "        preds = pl_module.non_linear_eval(rep)\n",
    "        loss = F.cross_entropy(preds,y)\n",
    "        \n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        acc = Accuracy()(preds,y)\n",
    "        self.log('ssl_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        self.log('ssl_acc', acc, prog_bar=True, on_epoch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_finetuner = SSLOnlineEvaluator(z_dim=2048 * 2 * 2, num_classes = 5)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(monitor='ssl_loss',\n",
    "                                          save_top_k=1,\n",
    "                                          save_last=False,\n",
    "                                          save_weights_only=True,\n",
    "                                          filename='simCLR',\n",
    "                                          verbose=True,\n",
    "                                          mode='min')\n",
    "\n",
    "lr_logger = LearningRateMonitor()\n",
    "\n",
    "callbacks = [online_finetuner, lr_logger, checkpoint_callback, online_finetuner]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "train_samples = len(dm.train_dataloader())\n",
    "\n",
    "model = SimCLR(batch_size=batch_size, num_samples=train_samples)\n",
    "trainer = pl.Trainer(callbacks=callbacks, gpus=0, max_epochs=1, fast_dev_run=True)\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
